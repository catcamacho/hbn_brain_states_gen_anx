{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcb78b5-894f-42b5-82e2-b1f4d4dfb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import scipy.stats as scp\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import pickle\n",
    "from subprocess import check_call\n",
    "import os\n",
    "\n",
    "sns.set(context='talk',style='white')\n",
    "\n",
    "hbn_folder = '/Users/catcamacho/Library/CloudStorage/Box-Box/CCP/HBN_study'\n",
    "project_folder = '/Users/catcamacho/Library/CloudStorage/Box-Box/SEAL/stickystates'\n",
    "data_folder = os.path.join(project_folder, 'DATA','hbn')\n",
    "ts_folder = os.path.join(data_folder, 'sub')\n",
    "atlas_folder = os.path.join(hbn_folder, 'proc','null_lL_WG33')\n",
    "out_folder = os.path.join(project_folder, 'ANALYSIS','hbn_brain_states_final', 'otheratlases')\n",
    "medial_wall = os.path.join(atlas_folder, 'Human.MedialWall_Conte69.32k_fs_LR.dlabel.nii')\n",
    "\n",
    "sample_file = os.path.join(hbn_folder,'social_proc_networks','dynamic_connectivity','DATA', \n",
    "                           'helper_files','sample_gord.32k_fs_LR.pscalar.nii')\n",
    "\n",
    "# get parcel and network labels\n",
    "parcel_labels = nib.load(sample_file).header.get_axis(1).name\n",
    "network_labels = []\n",
    "for s in parcel_labels:\n",
    "    b = s.split('_')\n",
    "    if len(b)<2:\n",
    "        network_labels.append(b[0])\n",
    "    else:\n",
    "        network_labels.append(b[1])\n",
    "network_labels = np.array(network_labels)\n",
    "network_names, network_sizes = np.unique(network_labels, return_counts=True)\n",
    "\n",
    "# define measures of interest\n",
    "atlasnames = ['Yeo7networks','Yeo17networks','Power_Neuron11']\n",
    "networks_of_interest = ['Auditory', 'CinguloOperc', 'Default', 'DorsalAttn', 'FrontoParietal',\n",
    "                        'SMhand', 'SMmouth', 'Salience', 'VentralAttn', 'Visual']\n",
    "networks_palette = ['#FF00FF','#800080','#FF0000','#00FF00','#FFFF00','#00FFFF','#FF8000',\n",
    "                    '#000000','#008080','#0000BD']\n",
    "\n",
    "features_of_interest = ['Positive','Negative','Anger','Happy','Fear','Sad','Excited','Brightness',\n",
    "                        'SaliencyFract','Sharpness','Vibrance','Loudness','Motion']\n",
    "timing = np.round(np.arange(0,600,0.8),1) # for DM\n",
    "\n",
    "state_palette = ['#7F7F7F','#05159b', '#cf28cf', '#008c8c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0e6d5ea-88ad-425e-874f-e3295affae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "# split dlabel file \n",
    "yeo_dlabel = nib.load(os.path.join(atlas_folder, 'RSN-networks_converted_resampled.32k_fs_LR.dlabel.nii'))\n",
    "gordon_dlabel = nib.load(os.path.join(atlas_folder, 'Gordon333_nosubcort.32k_fs_LR.dlabel.nii'))\n",
    "dlabel_axis = yeo_dlabel.header.get_axis(0)\n",
    "gbrain = gordon_dlabel.header.get_axis(1)\n",
    "\n",
    "for i, l in enumerate(dlabel_axis.label[:3]):\n",
    "    data = np.expand_dims(yeo_dlabel.get_fdata()[i,:], axis=0)\n",
    "    ax1 = yeo_dlabel.header.get_axis(1)\n",
    "\n",
    "    mwdata = nib.load(medial_wall).get_fdata()\n",
    "    data[mwdata==1] = 0\n",
    "    \n",
    "    if 'Power' not in atlasnames[i]:\n",
    "        lnew = {}\n",
    "        \n",
    "        for x in np.unique(data):\n",
    "            if x!=37:\n",
    "                lnew[x] = l[x]\n",
    "        ax0 = nib.cifti2.LabelAxis(name=np.array([atlasnames[i]]), label=lnew, meta=[{}])\n",
    "        \n",
    "    else:\n",
    "        newvals = [16, 4, 14, 3, 15, 6, 9]\n",
    "        oldvals = [27, 28, 29, 31, 32, 33, 62]\n",
    "        for j, x in enumerate(oldvals):\n",
    "            data[data==x] = newvals[j]\n",
    "        l = {\n",
    "        0: ('???', (0.667, 0.667, 0.667, 0.0)),\n",
    "        3: ('Default_mode', (1.0, 0.0, 0.0, 1.0)),\n",
    "        4: ('Hand_somatosensory-motor', (0.0, 1.0, 1.0, 1.0)),\n",
    "        5: ('Visual', (0.0, 0.0, 1.0, 1.0)),\n",
    "        6: ('Fronto-parietal', (0.961, 0.961, 0.059, 1.0)),\n",
    "        7: ('Ventral_attention', (0.0, 0.502, 0.502, 1.0)),\n",
    "        9: ('Language_Superior_temporal_gyrus', (1.0, 0.722, 0.827, 1.0)),\n",
    "        14: ('Cingulo-opercular', (0.502, 0.0, 0.502, 1.0)),\n",
    "        15: ('Dorsal_attention', (0.0, 0.863, 0.0, 1.0)),\n",
    "        16: ('Mouth_somatosensory-motor', (1.0, 0.502, 0.0, 1.0)),\n",
    "        20: ('Salience', (0.0, 0.0, 0.0, 1.0)),\n",
    "        24: ('Auditory', (1.0, 0.0, 1.0, 1.0))\n",
    "        }\n",
    "        newvals = l.keys()\n",
    "        for x in np.unique(data):\n",
    "            if x not in newvals:\n",
    "                data[data==x] = 0\n",
    "        ax0 = nib.cifti2.LabelAxis(name=np.array([atlasnames[i]]), label=l, meta=[{}])\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(data, (ax0, ax1))\n",
    "    nib.save(img, os.path.join(atlas_folder, '{0}.32k_fs_LR.dlabel.nii'.format(atlasnames[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baebaa8-e887-409d-8bd3-7b40bb10a5ee",
   "metadata": {},
   "source": [
    "# parcellate and compile data using Yeo and Power networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "874a5834-0052-49a3-8c70-09ce2cddafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movieDM.csv'), index_col=0)\n",
    "\n",
    "for atlas in atlasnames:\n",
    "    atlas_file = os.path.join(atlas_folder, '{0}.32k_fs_LR.dlabel.nii'.format(atlas))\n",
    "    for sub in subinfo.index:\n",
    "        outfile = os.path.join(ts_folder,'{0}_task-movieDM_bold1_AP_Atlas_demean_detrend_resid0.9_filt_{1}.32k_fs_LR.ptseries.nii'.format(sub,atlas))\n",
    "        infile = os.path.join(ts_folder,'{0}_task-movieDM_bold1_AP_Atlas_demean_detrend_resid0.9_filt.32k_fs_LR.dtseries.nii'.format(sub))\n",
    "        if (not os.path.isfile(outfile)):\n",
    "            check_call(['wb_command','-cifti-parcellate',infile, atlas_file, 'COLUMN', outfile, '-only-numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5d1d057-ee0f-4366-8f57-7b258a7818a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile data from 7 brain regions measured at 750 timepoints from 620 participants.\n",
      "Compile data from 17 brain regions measured at 750 timepoints from 620 participants.\n",
      "Compile data from 11 brain regions measured at 750 timepoints from 620 participants.\n"
     ]
    }
   ],
   "source": [
    "def compile_ts_data(subdf, movie, atlas, datadir, outfile, motionfile):\n",
    "    \"\"\"\n",
    "    combine data for each movie together into 1 file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subdf: DataFrame\n",
    "        A dataframe with subject IDs as the index. Includes IDs for all usable data.\n",
    "    movie: str\n",
    "        Corresponds with the str for the movie content to concatenate (e.g., \"DM\" or \"TP\").\n",
    "    atlas:\n",
    "    datadir: folder path\n",
    "        Path to folder with the subject timeseries ciftis.\n",
    "    outfile: file path\n",
    "        Path including filename to save the output data of shape Ntimepoints x Nparcels x Nsubjects.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data: numpy array\n",
    "        The compiled data of shape Ntimepoints x Nparcels x Nsubjects\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(subdf, pd.DataFrame):\n",
    "        subdf = pd.read_csv(subdf, index_col=0)\n",
    "    \n",
    "    for sub in subdf.index:\n",
    "        file = '{0}/{1}_task-movie{2}_bold1_AP_Atlas_demean_detrend_resid0.9_filt_{3}.32k_fs_LR.ptseries.nii'.format(datadir,sub, movie, atlas)\n",
    "        #motion = np.loadtxt('{0}/{1}/{1}_task-movie{2}_bold1_AP_FD.txt'.format(datadir,sub, movie))\n",
    "        if sub == subdf.index[0]:\n",
    "            data = StandardScaler().fit_transform(nib.load(file).get_fdata())\n",
    "            data = np.expand_dims(data, axis=2)\n",
    "            #motiondata = np.expand_dims(motion, axis=1)\n",
    "        else:\n",
    "            t = StandardScaler().fit_transform(nib.load(file).get_fdata())\n",
    "            t = np.expand_dims(t, axis=2)\n",
    "            #motion = np.expand_dims(motion, axis=1)\n",
    "            data = np.concatenate([data,t],axis=2)\n",
    "            #motiondata = np.concatenate([motiondata,motion],axis=1)\n",
    "    \n",
    "    print('Compile data from {0} brain regions measured at {1} timepoints from {2} participants.'.format(data.shape[1],data.shape[0],data.shape[2]))\n",
    "    np.save(outfile, data)\n",
    "    #np.save(motionfile, motiondata)\n",
    "\n",
    "\n",
    "subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movieDM.csv'), index_col=0)\n",
    "motionfile = os.path.join(data_folder, 'compiled_motion_data_movieDM.npy')\n",
    "\n",
    "for atlas in atlasnames:\n",
    "    outfile = os.path.join(data_folder, 'compiled_ts_data_movieDM_{0}.npy'.format(atlas))\n",
    "    compile_ts_data(subinfo, 'DM', atlas, ts_folder, outfile, motionfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd48d2-877b-4099-96a7-de5987634696",
   "metadata": {
    "tags": []
   },
   "source": [
    "# train model on Despicable Me and find optimal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a422db-49ec-4153-b493-23bd0e7caba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie='DM'\n",
    "\n",
    "for atlas in atlasnames:\n",
    "    subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movie{0}.csv'.format(movie)), index_col=0)\n",
    "    func_file = os.path.join(data_folder, 'compiled_ts_data_movie{0}_{1}.npy'.format(movie, atlas))\n",
    "    motion = np.load(os.path.join(data_folder, 'compiled_motion_data_movie{0}.npy'.format(movie)))\n",
    "    func_data = np.load(func_file)\n",
    "    net_func_data = np.empty((func_data.shape[0], len(networks_of_interest), func_data.shape[2]))\n",
    "    for i, n in enumerate(networks_of_interest):\n",
    "        net_func_data[:,i,:] = np.mean(func_data[:,network_labels==n,:], axis=1)\n",
    "    \n",
    "    # split data into discovery and replication\n",
    "    disc_data = net_func_data[:,:, subinfo['site']=='rubic']\n",
    "    disc_motion = motion[:, subinfo['site']=='rubic']\n",
    "    disc_info = subinfo.loc[subinfo['site']=='rubic',:]\n",
    "    \n",
    "    t_concat_disc = []\n",
    "    for i in range(0, disc_data.shape[2]):\n",
    "        m = disc_motion[:,i]\n",
    "        t = disc_data[:,:,i]\n",
    "        t_concat_disc.append(t[m<0.9,:])\n",
    "    t_concat_disc = np.concatenate(t_concat_disc, axis=0)\n",
    "    \n",
    "    rep_data = net_func_data[:,:, subinfo['site']=='cbic']\n",
    "    rep_motion = motion[:, subinfo['site']=='cbic']\n",
    "    rep_info = subinfo.loc[subinfo['site']=='cbic',:]\n",
    "    \n",
    "    t_concat_rep = []\n",
    "    for i in range(0, rep_data.shape[2]):\n",
    "        t = rep_data[:,:,i]\n",
    "        m = rep_motion[:,i]\n",
    "        t_concat_rep.append(t[m<0.9,:])\n",
    "    t_concat_rep = np.concatenate(t_concat_rep, axis=0)\n",
    "    \n",
    "    # save datasets\n",
    "    np.save(os.path.join(data_folder, 'movie{0}_disc_concat_data_{1}.npy'.format(movie, atlas)), t_concat_disc)\n",
    "    np.save(os.path.join(data_folder, 'movie{0}_rep_concat_data_{1}.npy'.format(movie, atlas)), t_concat_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f9ce9-b352-4394-bc4c-222daf33716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = pd.DataFrame()\n",
    "\n",
    "for k in range(2,20):\n",
    "    print(k)\n",
    "    model = hmm.GMMHMM(n_components=k, covariance_type=\"full\", n_iter=10000)\n",
    "    model.fit(t_concat_disc)\n",
    "\n",
    "    # save scores\n",
    "    train_score = model.score(t_concat_disc)\n",
    "    test_score, _ = model.decode(t_concat_rep)\n",
    "    n_params = model.covars_.shape[0]*model.covars_.shape[1]*model.covars_.shape[2]\n",
    "    test_a = -2*test_score + 2*n_params\n",
    "    test_b = -2*test_score + n_params*np.log(t_concat_rep.shape[0])\n",
    "\n",
    "    model_fit.loc[k, 'n_states'] = k\n",
    "    model_fit.loc[k,'train_ll'] = train_score\n",
    "    model_fit.loc[k,'test_ll'] = test_score\n",
    "    model_fit.loc[k,'test_aic'] = test_a\n",
    "    model_fit.loc[k,'test_bic'] = test_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4001a3-fca2-4576-9215-0a6d0581ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95392591-3ed6-469e-9cb2-6cd86d56f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit_d = model_fit.diff()\n",
    "model_fit_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d236c5-064b-4671-87f8-6da9fdef5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_fit['n_states'],model_fit['train_ll'])\n",
    "plt.savefig(os.path.join(out_folder,'movieDM_train_model_fit_plot.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(model_fit['n_states'],model_fit['test_ll'])\n",
    "plt.savefig(os.path.join(out_folder,'movieDM_test_model_fit_plot.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(model_fit['n_states'],model_fit['test_aic'])\n",
    "plt.savefig(os.path.join(out_folder,'movieDM_test_model_fit_aic_plot.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(model_fit['n_states'],model_fit['test_bic'])\n",
    "plt.savefig(os.path.join(out_folder,'movieDM_test_model_fit_bic_plot.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "model_fit.to_csv(os.path.join(out_folder,'movieDM_model_fit_stats.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2843bac-58c9-4750-9fdc-1f0fea563811",
   "metadata": {},
   "source": [
    "## fit and save optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9d239-8564-4ca8-871a-4b79ff2b5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie='DM'\n",
    "nstates=3\n",
    "t_concat_disc = np.load(os.path.join(data_folder, 'movie{0}_disc_concat_data.npy'.format(movie)))\n",
    "\n",
    "# fit and save optimal model\n",
    "model = hmm.GMMHMM(n_components=nstates, covariance_type=\"full\", n_iter=10000)\n",
    "model.fit(t_concat_disc)\n",
    "\n",
    "# save outputs\n",
    "pickle.dump(model, open(os.path.join(out_folder,'movieDM_k{0}_model.pkl'.format(nstates)), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4398f-dbd5-4d1a-887b-30867a8a7bdb",
   "metadata": {},
   "source": [
    "#### Compute CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ba320-b606-4a6d-8b23-58d5c6b96acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and set up model\n",
    "movie='DM'\n",
    "nstates=3\n",
    "train_data_file = os.path.join(data_folder, 'movie{0}_disc_concat_data.npy'.format(movie))\n",
    "model = pickle.load(open(os.path.join(out_folder,'movieDM_k{0}_model.pkl'.format(nstates)), 'rb'))\n",
    "\n",
    "# bootstrapping parameters\n",
    "nboots = 1000\n",
    "lowern = 0.1\n",
    "uppern = 0.3\n",
    "ci = 95\n",
    "samplen = np.load(train_data_file).shape[0]\n",
    "\n",
    "boot_results = np.empty((nstates, nstates, nboots))\n",
    "\n",
    "for i in range(nboots):\n",
    "    # subset data\n",
    "    bootsample_size = random.randint(int(samplen*lowern),int(samplen*uppern))\n",
    "    subsampmask = np.full(samplen, 0)\n",
    "    subsampmask[:bootsample_size] = 1\n",
    "    start = random.randint(0,samplen-int(samplen*uppern))\n",
    "    subsampmask = np.roll(subsampmask, start)\n",
    "    temp = np.load(train_data_file)[subsampmask==1,:]\n",
    "    res = model.decode(temp)[1]\n",
    "    \n",
    "    # fit the model\n",
    "    transitions = np.zeros((nstates, nstates))\n",
    "    for j in range(res.shape[0]-1):\n",
    "        s = res[j]\n",
    "        sn = res[j+1]\n",
    "        transitions[s,sn] = transitions[s,sn]+1\n",
    "    marg0_subtra = np.sum(transitions, axis=0)\n",
    "    boot_results[:,:,i] = transitions/marg0_subtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e4001-fc4c-44d2-9820-6d24d56e1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(boot_results, axis=2)\n",
    "lowerCI = np.percentile(boot_results, (100-ci)/2, axis=2, keepdims=True)\n",
    "upperCI = np.percentile(boot_results, 100-((100-ci)/2), axis=2, keepdims=True)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for a in range(nstates):\n",
    "    for b in range(nstates):\n",
    "        results.loc['s{0}_s{1}'.format(a+1,b+1), 'mean'] = means[a,b]\n",
    "        results.loc['s{0}_s{1}'.format(a+1,b+1), 'lowerCI'] = lowerCI[a,b]\n",
    "        results.loc['s{0}_s{1}'.format(a+1,b+1), 'upperCI'] = upperCI[a,b]\n",
    "results.to_csv(os.path.join(out_folder, 'transition_probabilities_CI_k3.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffa3c4-7d80-4658-8cb2-96e2b1407ccb",
   "metadata": {},
   "source": [
    "## Test model significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62231c-5b9e-4bbe-ad02-f674bf48b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates=3\n",
    "nperms=10000\n",
    "movie='DM'\n",
    "model_fit = pd.DataFrame()\n",
    "\n",
    "model = pickle.load(open(os.path.join(out_folder,'movieDM_k{0}_model.pkl'.format(nstates)), 'rb'))\n",
    "t_concat_rep = np.load(os.path.join(data_folder, 'movie{0}_rep_concat_data.npy'.format(movie)))\n",
    "origdims = t_concat_rep.shape\n",
    "\n",
    "for i in range(nperms):\n",
    "    temp = t_concat_rep.flatten()\n",
    "    np.random.shuffle(temp)\n",
    "    t_concat_rep_perm = np.reshape(temp, origdims)\n",
    "    test_score, _ = model.decode(t_concat_rep_perm)\n",
    "    n_params = model.covars_.shape[0]*model.covars_.shape[1]*model.covars_.shape[2]\n",
    "    test_a = -2*test_score + 2*n_params\n",
    "    test_b = -2*test_score + n_params*np.log(t_concat_rep.shape[0])\n",
    "\n",
    "    model_fit.loc[i,'test_ll'] = test_score\n",
    "    model_fit.loc[i,'test_aic'] = test_a\n",
    "    model_fit.loc[i,'test_bic'] = test_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658b3ed-2f39-4add-ade9-0477a125ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_fit = pd.read_csv(os.path.join(out_folder,'movieDM_model_fit_stats.csv'), index_col=0)\n",
    "\n",
    "for stat in model_fit.columns:\n",
    "    fig, ax = plt.subplots(figsize=(6,3))\n",
    "    ax.hist(model_fit.loc[:,stat], 20, label='Permutated Scores', density=False)\n",
    "    ax.axvline(actual_fit.loc[3,stat], ls='--', color='r', label='Actual Model Fit')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_folder, 'permutation_plot_{0}.png'.format(stat)), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62820717-8287-4637-b6c6-a457eb7bb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_stats = pd.DataFrame()\n",
    "for stat in model_fit.columns:\n",
    "    if stat!='test_ll':\n",
    "        pvalue=(np.sum((model_fit.loc[:,stat]<=actual_fit.loc[3,stat]).astype(int)) + 1) / (nperms + 1)\n",
    "        perm_stats.loc[stat, 'actual_stat'] = actual_fit.loc[3,stat]\n",
    "        perm_stats.loc[stat, 'perm_pval'] = pvalue\n",
    "    else:\n",
    "        pvalue=(np.sum((model_fit.loc[:,stat]>=actual_fit.loc[3,stat]).astype(int)) + 1) / (nperms + 1)\n",
    "        perm_stats.loc[stat, 'actual_stat'] = actual_fit.loc[3,stat]\n",
    "        perm_stats.loc[stat, 'perm_pval'] = pvalue\n",
    "model_fit.to_csv(os.path.join(out_folder, 'permuation_testing_permresults.csv'))\n",
    "perm_stats.to_csv(os.path.join(out_folder, 'permuation_testing_perm_stats.csv'))\n",
    "perm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8eb5b-f21f-44e0-a208-eae877eea1ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## apply model to all participant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d35056-fb63-4c28-8384-58c5fc3ab540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nstates=3\n",
    "movie='DM'\n",
    "model = pickle.load(open(os.path.join(out_folder,'movieDM_k{0}_model.pkl'.format(nstates)), 'rb'))\n",
    "\n",
    "subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movie{0}.csv'.format(movie)), index_col=0)\n",
    "net_func_data = np.load(os.path.join(data_folder, 'movie{0}_sample-full_net_data.npy'.format(movie)))\n",
    "func_data = np.load(os.path.join(data_folder, 'compiled_ts_data_movie{0}.npy'.format(movie)))\n",
    "\n",
    "# make mean activation maps\n",
    "ax0 = nib.load(sample_file).header.get_axis(0)\n",
    "ax1 = nib.load(sample_file).header.get_axis(1)\n",
    "\n",
    "mean_netact_df = pd.DataFrame()\n",
    "state_info_df = pd.DataFrame()\n",
    "sample_states = []\n",
    "state_prob = {0:pd.DataFrame(columns=timing, dtype=float, index=subinfo.index),\n",
    "              1:pd.DataFrame(columns=timing, dtype=float, index=subinfo.index),\n",
    "              2:pd.DataFrame(columns=timing, dtype=float, index=subinfo.index),\n",
    "              3:pd.DataFrame(columns=timing, dtype=float, index=subinfo.index)}\n",
    "ind = 0\n",
    "\n",
    "mean_act = {'s0':[],'s1':[], 's2':[],'s3':[]}\n",
    "\n",
    "for i, sub in enumerate(subinfo.index):\n",
    "    sub_data = np.squeeze(nib.load(os.path.join(ts_folder, sub, '{0}_task-movieDM_bold1_AP_Atlas_demean_detrend_resid0.9_filt_gordon.32k_fs_LR.ptseries.nii'.format(sub))).get_fdata())\n",
    "    sub_data = StandardScaler().fit_transform(sub_data)\n",
    "    net_func_data = np.empty((sub_data.shape[0], len(networks_of_interest)))\n",
    "    for i, n in enumerate(networks_of_interest):\n",
    "        net_func_data[:,i] = np.mean(sub_data[:,network_labels==n], axis=1)\n",
    "    motion = np.loadtxt(os.path.join(ts_folder, sub, '{0}_task-movieDM_bold1_AP_FD.txt'.format(sub)))\n",
    "    res = np.zeros((net_func_data.shape[0],))\n",
    "    res[motion<0.9] = model.decode(net_func_data[motion<0.9,:])[1] + 1\n",
    "    sample_states.append(np.expand_dims(res,0))\n",
    "    prob = model.predict_proba(net_func_data)\n",
    "    prob[motion>=0.9,:] = np.nan\n",
    "    np.save(os.path.join(out_folder,'sub','{0}_movie{1}_k{2}_states.npy'.format(sub, movie, nstates)), res)\n",
    "    np.save(os.path.join(out_folder,'sub','{0}_movie{1}_k{2}_state_probabilities.npy'.format(sub, movie, nstates)), prob)\n",
    "    for s in range(1, nstates+1): \n",
    "        mask = res==s\n",
    "        state_info_df.loc[sub,'s{0}_percent'.format(s)] = np.nanmean(mask)*100\n",
    "        if s in res:\n",
    "            state_prob[s].loc[sub,:] = prob[:,s-1]\n",
    "            pa = np.mean(sub_data[mask,:], axis=0)\n",
    "            mean_act['s{0}'.format(s)].append(np.expand_dims(pa, axis=1))\n",
    "            na=net_func_data[mask,:]\n",
    "            mean_netact_df.loc[ind,'sub'] = sub\n",
    "            mean_netact_df.loc[ind,'state'] = s\n",
    "            mean_netact_df.loc[ind,networks_of_interest] = np.mean(na, axis=0)\n",
    "            ind = ind + 1\n",
    "group_data = np.concatenate(sample_states, axis=0)\n",
    "group_data = np.squeeze(group_data)\n",
    "np.save(os.path.join(out_folder,'group-movie{0}_k{1}_states.npy'.format(movie, nstates)), group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda2028-64af-4ae4-a08a-f49f9c48d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(1,nstates+1):\n",
    "    mean_act_data = np.mean(np.concatenate(mean_act['s{0}'.format(s)],axis=1), axis=1)\n",
    "    img = nib.cifti2.cifti2.Cifti2Image(np.expand_dims(mean_act_data, axis=0), (ax0, ax1))\n",
    "    nib.save(img, os.path.join(out_folder, 'movie{0}_k{2}_state{1}.32k_fs_LR.pscalar.nii'.format(movie, s, nstates)))\n",
    "\n",
    "state_info_df.to_csv(os.path.join(out_folder, 'movie{0}_states_info.csv'.format(movie)))\n",
    "mean_netact_df.to_csv(os.path.join(out_folder, 'movie{0}_mean_netact.csv'.format(movie)))\n",
    "pickle.dump(mean_act, open(os.path.join(out_folder,'movie{0}_k{1}_state_activation.pkl'.format(movie, nstates)), 'wb'))\n",
    "pickle.dump(state_prob, open(os.path.join(out_folder,'movie{0}_k{1}_state_probabilities.pkl'.format(movie, nstates)), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1a7f9-30ad-4208-9047-dbd830b19ac8",
   "metadata": {},
   "source": [
    "## Compute individual transition odds ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee72e95-9226-4b87-ae87-79f8c9ae5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = pd.DataFrame()\n",
    "subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movieDM.csv'), index_col=0)\n",
    "for sub in subinfo.index:\n",
    "    substates = np.load(os.path.join(out_folder,'sub','{0}_movieDM_k{1}_states.npy'.format(sub, nstates))).astype(int)\n",
    "    # overall video\n",
    "    subtransitions = np.zeros((nstates+1, nstates+1))\n",
    "    for i in range(1, substates.shape[0]-1):\n",
    "        s = substates[i]\n",
    "        sn = substates[i+1]\n",
    "        subtransitions[s,sn] = subtransitions[s,sn]+1\n",
    "    marg0_subtra = np.sum(subtransitions, axis=0)\n",
    "    odds = subtransitions/marg0_subtra\n",
    "    states = np.arange(1,nstates+1).astype(int).tolist()\n",
    "    for s1 in states:\n",
    "        for s2 in states:\n",
    "            odds_ratios.loc[sub,'ntrans_s{0}_to_{1}_all'.format(s1,s2)] = subtransitions[s1,s2]\n",
    "    for s1 in states:\n",
    "        for s2 in states:\n",
    "            odds_ratios.loc[sub,'s{0}_to_{1}_all'.format(s1,s2)] = odds[s1,s2]\n",
    "    for i in range(1, nstates+1):\n",
    "        states = np.arange(1,nstates+1).astype(int).tolist()\n",
    "        states.remove(i)\n",
    "        odds_ratios.loc[sub,'s{0}_to_{1}vs{2}_ratio_all'.format(i, states[0],states[1])] = odds[i,states[0]]/odds[i,states[1]]\n",
    "        odds_ratios.loc[sub,'s{0}_to_{1}vs{2}_ratio_all'.format(i, states[1],states[0])] = odds[i,states[1]]/odds[i,states[0]]\n",
    "odds_ratios = odds_ratios.replace(np.inf,np.nan)\n",
    "odds_ratios.to_csv(os.path.join(out_folder, 'movieDM_sample_oddsratios.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3babbc-a8e7-4926-b1f7-c45a84c077b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac4380-b54e-4ac8-9dd3-58fc09f0a55c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e90f94-bd5f-4f4f-9015-0c7612e868ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overall brain states for each movie\n",
    "state_palette = ['#05159b', '#cf28cf', '#008c8c']\n",
    "nstates=3\n",
    "group_data = np.load(os.path.join(out_folder,'group-movieDM_k{0}_states.npy'.format(nstates)))\n",
    "\n",
    "\n",
    "# plot states across the sample\n",
    "sample_res_df = pd.DataFrame(group_data, columns=timing)\n",
    "sns.set(context='talk', style='white')\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(sample_res_df, cmap=['#7F7F7F']+state_palette)\n",
    "plt.savefig(os.path.join(out_folder,'group-movieDM_k{0}_states_sample.png'.format(nstates)), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883bf5e-e354-49fd-9eee-9a1513877bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = np.load(os.path.join(out_folder,'group-movieDM_k3_states.npy'))\n",
    "sample_res_df = pd.DataFrame(np.squeeze(group_data), columns=timing)\n",
    "\n",
    "states_counts_df = pd.DataFrame()\n",
    "for i in sample_res_df.columns:\n",
    "    states_counts_df.loc[i,'s0'] = sum(sample_res_df[i]==0)\n",
    "    states_counts_df.loc[i,'s1'] = sum(sample_res_df[i]==1)\n",
    "    states_counts_df.loc[i,'s2'] = sum(sample_res_df[i]==2)\n",
    "    states_counts_df.loc[i,'s3'] = sum(sample_res_df[i]==3)\n",
    "\n",
    "data_perc = states_counts_df.divide(states_counts_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Make the plot\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.stackplot(data_perc.index, data_perc[\"s0\"], data_perc[\"s1\"],  data_perc[\"s2\"], data_perc[\"s3\"],\n",
    "              labels=[ 'Motion', 'HighVIS', 'HighCON','HighVAN'], \n",
    "              colors=['#7F7F7F']+ state_palette)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.xlim(0,timing[-1])\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Time (s)')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_folder,'group-movieDM_k3_states_density.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33ce30-1afa-4613-9bcd-a49723845134",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot mean network activation per state\n",
    "mean_netact_df = pd.read_csv(os.path.join(out_folder, 'movieDM_mean_netact.csv'), index_col=0)\n",
    "for s in range(1,6):\n",
    "    t = mean_netact_df.loc[mean_netact_df['state']==s]\n",
    "    t = pd.melt(t, value_vars=networks_of_interest, var_name='network', value_name='mean_activation') \n",
    "\n",
    "    plt.figure()\n",
    "    sns.catplot(data=t, kind='boxen',ci=None,x='network', y='mean_activation', \n",
    "                palette=networks_palette, height=8)\n",
    "    #sns.stripplot(data=t,x='network', y='mean_activation',color='gray', alpha=0.1)\n",
    "    plt.axhline(y=0, color='k', linestyle='-')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('State {0}'.format(s))\n",
    "    plt.ylabel('Activation (arbitrary units)')\n",
    "    plt.xlabel('Network')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_folder,'group-movieDM_k5_state{0}_netmean_boxen.png'.format(s)), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed989e-8fac-4629-9c7f-07d87ee4ca0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35dc766-46b2-4032-a23e-5d5a624797bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute sig net activation/deactivation\n",
    "\n",
    "mean_netact_df = pd.read_csv(os.path.join(out_folder, 'movieDM_mean_netact.csv'), index_col=0)\n",
    "net_activation_stats = pd.DataFrame()\n",
    "i = 0\n",
    "for s in range(1,6):\n",
    "    t = mean_netact_df.loc[mean_netact_df['state']==s]\n",
    "    for net in networks_of_interest:\n",
    "        tstat, pval = scp.ttest_1samp(t.loc[:,net], 0)\n",
    "        net_activation_stats.loc[i, 'State'] = s\n",
    "        net_activation_stats.loc[i, 'Network'] = net\n",
    "        net_activation_stats.loc[i, 't-stat'] = tstat\n",
    "        net_activation_stats.loc[i, 'pval'] = pval\n",
    "        net_activation_stats.loc[i, 'df'] = len(t.loc[:,net]) - 1\n",
    "        net_activation_stats.loc[i, 'sig'] = pval<0.05\n",
    "        net_activation_stats.loc[i, 'fdr_sig'] = pval<(0.05/len(networks_of_interest))\n",
    "        i = i + 1\n",
    "net_activation_stats.to_csv(os.path.join(out_folder, 'movieDM_k3_netact_ttests.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72abc80-4720-4691-9e23-69102306f520",
   "metadata": {},
   "source": [
    "# Relate brain states to movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbc56c-076b-4fc2-b6a6-4ede5fc4e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_smooth(data, time, sampling_rate, window=4):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy array\n",
    "        1-D array with signal data to smooth.\n",
    "    time: numpy array\n",
    "        Time stamps in seconds for the signals to be smoothed.\n",
    "    sampling_rate: float\n",
    "        The sampling rate in Hz that the data were acquired in.\n",
    "    window: int\n",
    "        The size of the gaussian kernel to use for smoothing (must be even number).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    smoothed: numpy array\n",
    "        1-D array with smoothed data.\n",
    "    \n",
    "    \"\"\"\n",
    "    def gaussian(t, fwhm):\n",
    "        return np.exp(-(4*np.log(2)*t**2)/fwhm**2)\n",
    "\n",
    "    # create kernel\n",
    "    n = len(time)\n",
    "    k = int(window/2)\n",
    "    gtime = np.arange(-k, k)/sampling_rate\n",
    "\n",
    "    gauswin = gaussian(gtime, window)\n",
    "    gauswin = gauswin/np.sum(gauswin)\n",
    "    \n",
    "    # zeropad the data\n",
    "    pad_data = np.pad(data, (window,window), mode='constant', constant_values=0)\n",
    "    \n",
    "    # smooth data\n",
    "    smoothed = np.zeros_like(pad_data)\n",
    "    for i in range(k+1, n-k-1):\n",
    "        smoothed[i] = np.sum(pad_data[i-k:i+k] * gauswin)\n",
    "    # remove pad\n",
    "    smoothed = smoothed[window:-window]\n",
    "    return(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d3ef7-306b-428e-9aea-494cf540b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie='DM'\n",
    "mean_prob = pd.read_csv(os.path.join(out_folder, 'movie{0}_k3_meanprobs.csv'.format(movie)), index_col=0)\n",
    "\n",
    "features = pd.read_csv(os.path.join(hbn_folder, 'HBN_video_coding','processing','v1','summary',\n",
    "                                    '{0}_summary_codes_intuitivenames.csv'.format(movie)), index_col=0)\n",
    "for f in features.columns:\n",
    "    features.loc[:,f] = temporal_smooth(features.loc[:,f], timing, 1.2)\n",
    "features = features.iloc[:-5,:]\n",
    "mean_prob_trim = mean_prob.iloc[5:,:]\n",
    "mean_prob_trim.columns = ['High_VIS','High_CON','High_VAN']\n",
    "mean_prob_trim.index = features.index\n",
    "combo = features.merge(mean_prob_trim, how='left', left_index=True, right_index=True)\n",
    "combo[['Positive','Negative','Brightness','Motion','SaliencyFract', 'High_VIS','High_CON',\n",
    "       'High_VAN']].plot(subplots=True, figsize=(14,12), xlim=(0,timing[-6]), \n",
    "                                  color=['k','k','k','k','k']+state_palette)\n",
    "sns.despine()\n",
    "plt.savefig(os.path.join(out_folder,'group-movie{0}_k3_mean_probabilities_features.png'.format(movie)), dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "c = combo.corr(method='spearman')\n",
    "plt.figure(figsize=(8,10))\n",
    "sns.heatmap(c.loc[['Positive','Negative','Brightness','Motion','SaliencyFract','Loudness'],\n",
    "                  ['High_VIS','High_CON','High_VAN']], center=0, vmax=0.5, vmin=-0.5, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_folder,'group-movie{0}_k3_corr_probabilities_features.png'.format(movie)), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65a8d0-6cb1-4704-9e7b-031d0d367b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['Positive','Negative','Brightness','Motion','SaliencyFract','Loudness']:\n",
    "    for d in ['High_Sens','High_CON','High_VAN']:\n",
    "        r, p = scp.spearmanr(combo[c], combo[d])\n",
    "        print('{0} and {1} r = {2}, p = {3}'.format(c, d, round(r,2), round(p,3)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a205a88-e78b-4ec7-b698-38e9a051488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state info for during and outside of negative content\n",
    "movie='DM'\n",
    "subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movie{0}.csv'.format(movie)), index_col=0)\n",
    "\n",
    "#pull in features\n",
    "features = pd.read_csv(os.path.join(hbn_folder, 'HBN_video_coding','processing','v1','summary',\n",
    "                                    '{0}_summary_codes_intuitivenames.csv'.format(movie)), index_col=0)\n",
    "for f in features.columns:\n",
    "    features.loc[:,f] = temporal_smooth(features.loc[:,f], timing, 1.2)\n",
    "features = features.iloc[:-5,:]\n",
    "neg_mask = features['Negative']>1\n",
    "\n",
    "# pull in network level activation\n",
    "#func_net_data = np.load(os.path.join(data_folder, 'movie{0}_sample-full_net_data.npy'.format(movie)))[5:,:,:]\n",
    "\n",
    "# set up dataframe for outputs\n",
    "neg_df = pd.DataFrame()\n",
    "\n",
    "for i, sub in enumerate(subinfo.index):\n",
    "    sub_states = np.load(os.path.join(out_folder,'sub','{0}_movie{1}_k{2}_states.npy'.format(sub, movie, nstates)))[5:]\n",
    "    sub_probs = np.load(os.path.join(out_folder,'sub','{0}_movie{1}_k{2}_state_probabilities.npy'.format(sub, movie, nstates)))[5:,:]\n",
    "    for s in range(1,4): \n",
    "        # Get percent of time in each state\n",
    "        sub_neg_mask = (sub_states==s) & neg_mask\n",
    "        sub_nonneg_mask = (sub_states==s) & (neg_mask==False)\n",
    "        neg_df.loc[sub,'neg_s{0}_pct'.format(s)] = np.nanmean(sub_neg_mask)*100\n",
    "        neg_df.loc[sub,'nonneg_s{0}_pct'.format(s)] = np.nanmean(sub_nonneg_mask)*100\n",
    "        \n",
    "        # get mean probability for each state\n",
    "        neg_df.loc[sub,'neg_s{0}_prob'.format(s)] = np.nanmean(sub_probs[sub_neg_mask,:][:, s-1])\n",
    "        neg_df.loc[sub,'nonneg_s{0}_prob'.format(s)] = np.nanmean(sub_probs[sub_nonneg_mask,:][:, s-1])\n",
    "        \n",
    "        # get mean net activation for each state\n",
    "        #if s in sub_states:\n",
    "            #sub_data = func_net_data[:,:,i]\n",
    "            #neg_net = sub_data[sub_neg_mask,:]\n",
    "            #nonneg_net = sub_data[sub_nonneg_mask,:]\n",
    "            #sub_neg_nets_labels = ['neg_s{0}_{1}'.format(s, n) for n in networks_of_interest]\n",
    "            #sub_nonneg_nets_labels = ['nonneg_s{0}_{1}'.format(s, n) for n in networks_of_interest]\n",
    "            #neg_df.loc[sub, sub_neg_nets_labels] = np.mean(neg_net, axis=0)\n",
    "            #neg_df.loc[sub, sub_nonneg_nets_labels] = np.mean(nonneg_net, axis=0)\n",
    "\n",
    "# save data\n",
    "neg_df.to_csv(os.path.join(out_folder, 'movie{0}_neg_states_activation_info.csv'.format(movie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e412d7-1cfa-4e17-8563-9d8098b5f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "odds_ratios = pd.DataFrame()\n",
    "subinfo = pd.read_csv(os.path.join(data_folder, 'sampleinfo_movie{0}.csv'.format(movie)), index_col=0)\n",
    "\n",
    "#pull in features\n",
    "features = pd.read_csv(os.path.join(hbn_folder, 'HBN_video_coding','processing','v1','summary',\n",
    "                                    '{0}_summary_codes_intuitivenames.csv'.format(movie)), index_col=0)\n",
    "for f in features.columns:\n",
    "    features.loc[:,f] = temporal_smooth(features.loc[:,f], timing, 1.2)\n",
    "features = features.iloc[:-5,:]\n",
    "neg_mask = features['Negative']>1\n",
    "\n",
    "for sub in subinfo.index:\n",
    "    substates = np.load(os.path.join(out_folder,'sub','{0}_movie{1}_k3_states.npy'.format(sub, movie))).astype(int)\n",
    "    substates = substates[5:][neg_mask==True]\n",
    "    # overall video\n",
    "    subtransitions = np.zeros((nstates+1, nstates+1))\n",
    "    subdifftransitions = np.zeros((nstates+1, nstates+1))\n",
    "    for i in range(substates.shape[0]-1):\n",
    "        s = substates[i]\n",
    "        sn = substates[i+1]\n",
    "        subtransitions[s,sn] = subtransitions[s,sn]+1\n",
    "    marg0_subtra = np.sum(subtransitions, axis=0)\n",
    "    odds = subtransitions/marg0_subtra\n",
    "    states = [1, 2, 3]\n",
    "    for s1 in states:\n",
    "        for s2 in states:\n",
    "            odds_ratios.loc[sub,'s{0}_to_{1}_neg'.format(s1,s2)] = odds[s1,s2]\n",
    "            odds_ratios.loc[sub,'ntrans_s{0}_to_{1}_neg'.format(s1,s2)] = subtransitions[s1,s2]\n",
    "    for i in range(1, nstates+1):\n",
    "        states = [1, 2, 3]\n",
    "        states.remove(i)\n",
    "        odds_ratios.loc[sub,'s{0}_to_{1}vs{2}_ratio_neg'.format(i, states[0],states[1])] = odds[i-1,states[0]]/odds[i-1,states[1]]\n",
    "        odds_ratios.loc[sub,'s{0}_to_{1}vs{2}_ratio_neg'.format(i, states[1],states[0])] = odds[i-1,states[1]]/odds[i-1,states[0]]\n",
    "odds_ratios = odds_ratios.replace(np.inf,np.nan)\n",
    "odds_ratios.to_csv(os.path.join(out_folder, 'movie{0}_sample_oddsratios_neg.csv'.format(movie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f58a20-c61d-4122-98fb-c9b42ce1cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb7026-a5f7-44aa-9c2c-855ee1f9020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
